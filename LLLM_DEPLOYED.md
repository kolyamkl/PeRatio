# ğŸš€ LLLM Quick Start

## Start/Stop/Status

```bash
cd /Users/macbook/Desktop/TG_TRADE/backend/LLM_PEAR/LLM/LLLM

# Start service
./start_lllm_service.sh

# Check status
./status_lllm_service.sh

# Stop service
./stop_lllm_service.sh

# View live logs
tail -f /tmp/lllm_service.log
```

## Service is Running âœ…

Your LLLM signal generator is now running as a background service:
- Generates signals every **30 minutes**
- Uses **real OpenAI GPT-4o-mini** 
- Saves to `latest_signal.json`
- Backend reads signals automatically

## What's Deployed?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend (Vite + React)          âœ… Runningâ”‚
â”‚  Backend (FastAPI)                âœ… Runningâ”‚
â”‚  LLLM Signal Generator            âœ… Runningâ”‚ â† NEW!
â”‚  Ngrok                            âœ… Runningâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## How It Works

1. **LLLM Service** (background) â†’ Generates signals every 30 min
2. **Signal Files** (JSON) â†’ Saved to disk
3. **Backend API** â†’ Reads signals when users request
4. **Mini App** â†’ Users see and execute trades

## All Your Running Services

| Service | Status | Port | Command |
|---------|--------|------|---------|
| Frontend | âœ… Running | 5173 | `npm run dev` |
| Backend API | âœ… Running | 8000 | `uvicorn main:app` |
| LLLM Signal | âœ… Running | - | `run_signal_service.py` |
| Ngrok | âœ… Running | - | `ngrok http 5173` |

## Test It

1. Open your mini app
2. Click "Generate Trade" 
3. You should see a fresh signal from LLLM!

## Monitor Logs

```bash
# LLLM logs
tail -f /tmp/lllm_service.log

# Backend logs
# (check your backend terminal)

# View latest signal
cat /Users/macbook/Desktop/TG_TRADE/backend/LLM_PEAR/LLM/LLLM/latest_signal.json
```

## Troubleshooting

**If LLLM stops working:**
```bash
./stop_lllm_service.sh
./start_lllm_service.sh --mock  # Use mock mode to test
```

**If OpenAI rate limit hit:**
```bash
./stop_lllm_service.sh
./start_lllm_service.sh --interval 60  # Generate every hour instead
```

## Your Complete System

Everything is now deployed:
- âœ… React frontend with wallet integration
- âœ… FastAPI backend with Telegram bot
- âœ… LLLM signal generator (continuous)
- âœ… Ngrok tunnel for external access
- âœ… All connected and working!

ğŸ‰ **You're fully deployed!**
